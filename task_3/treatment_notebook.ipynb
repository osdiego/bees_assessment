{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### This notebook basically does what was done in ***Task 1***, but using the package created on ***Task 2***."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The imports are being executed only when needed\n",
    "import pandas as pd\n",
    "from pprint import pprint"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Basic configuration of the notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configure the settings of pandas so the DataFrames can be better printed\n",
    "pd.set_option('display.max_rows', 500)\n",
    "pd.set_option('display.max_columns', 500)\n",
    "pd.set_option('display.width', 1000)\n",
    "pd.set_option('max_colwidth', 400)\n",
    "\n",
    "# The assets folder, where all files common to the project are saved\n",
    "assets_folder = '../assets/'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 1: read CSV files into pandas DataFrames to better understanding the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_bg_map = pd.read_csv(f'{assets_folder}background_mapping.csv')\n",
    "clean_columns = {c: c.strip() for c in df_bg_map}\n",
    "df_bg_map.rename(columns=clean_columns, inplace=True)\n",
    "\n",
    "df_bg_1 = pd.read_csv(f'{assets_folder}project_1_background.csv', index_col=0)\n",
    "df_bg_2 = pd.read_csv(f'{assets_folder}project_2_background.csv', index_col=0)\n",
    "\n",
    "df_logs_1 = pd.read_csv(f'{assets_folder}project_1_logs.csv', index_col=0)\n",
    "df_logs_2 = pd.read_csv(f'{assets_folder}project_2_logs.csv', index_col=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 2: check the difference between datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Background schemas are equal? Answer: False\n"
     ]
    }
   ],
   "source": [
    "# Verify the difference between the background df schemas\n",
    "is_bg_schemas_equal = sorted(list(df_bg_1)) == sorted(list(df_bg_2))\n",
    "print(f'Background schemas are equal? Answer: {is_bg_schemas_equal}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The columns in the mapping represents all the mismatched columns in background DataFrames: True\n"
     ]
    }
   ],
   "source": [
    "\n",
    "bg1_not_in_bg2 = sorted([c for c in df_bg_1 if c not in df_bg_2])\n",
    "bg2_not_in_bg1 = sorted([c for c in df_bg_2 if c not in df_bg_1])\n",
    "\n",
    "# Verify that the difference between dfs are the ones in the mapping\n",
    "prj_2_cols = sorted(df_bg_map['Project 1 Question'].values.tolist())\n",
    "is_prj_2_cols_mapped = prj_2_cols == bg2_not_in_bg1\n",
    "prj_1_cols = sorted(df_bg_map['Project 2 Question'].values.tolist())\n",
    "is_prj_1_cols_mapped = prj_1_cols == bg1_not_in_bg2\n",
    "\n",
    "# and that the columns in the mapping file are indeed switched\n",
    "is_all_columns_mapped = is_prj_2_cols_mapped and is_prj_1_cols_mapped\n",
    "print(f'The columns in the mapping represents all the mismatched columns in background DataFrames: {is_all_columns_mapped}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logs schemas are equal? Answer: True\n"
     ]
    }
   ],
   "source": [
    "# Confirm that the log DataFrames has the same schema\n",
    "is_logs_schemas_equal = sorted(list(df_logs_1)) == sorted(list(df_logs_2))\n",
    "print(f'Logs schemas are equal? Answer: {is_logs_schemas_equal}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 3: solve problem on column \"level2dish_coded\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sbees.insecure import clean_dict_column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['{\\'dish\\' : [\"Apples and pears\"]}',\n",
      " '{\\'dish\\' : [\"Berries / cherries / grapes\"]}',\n",
      " '{\\'dish\\' : [\"Biscuits with filling\"]}',\n",
      " '{\\'dish\\' : [\"Biscuits\"]}',\n",
      " '{\\'dish\\' : [\"Bread/ roll/ baguette\", \"Cured meats\", \"Hard cheese\"]}',\n",
      " '{\\'dish\\' : [\"Bread/ roll/ baguette\", \"Honey / syrup\"]}',\n",
      " '{\\'dish\\' : [\"Bread/ roll/ baguette\", \"Instant noodles\"]}',\n",
      " '{\\'dish\\' : [\"Bread/ roll/ baguette\"]}',\n",
      " '{\\'dish\\' : [\"Broccolli, cabbage, cauliflower\"]}',\n",
      " '{\\'dish\\' : [\"Cake\"]}',\n",
      " '{\\'dish\\' : [\"Candy\"]}',\n",
      " '{\\'dish\\' : [\"Chicken\", \"Other grains\"]}',\n",
      " '{\\'dish\\' : [\"Chocolate bar\"]}',\n",
      " '{\\'dish\\' : [\"Chocolate sweets/ snacks\"]}',\n",
      " '{\\'dish\\' : [\"Cured sausage\", \"Soft cheese/ cheese spread\", \"Bread/ roll/ '\n",
      " 'baguette\", \"Egg-based dish\"]}',\n",
      " '{\\'dish\\' : [\"Custards/ sweet puddings\"]}',\n",
      " '{\\'dish\\' : [\"Dried fruit\"]}',\n",
      " '{\\'dish\\' : [\"Egg-based dish\", \"Bread/ roll/ baguette\", \"Soft cheese/ cheese '\n",
      " 'spread\", \"Honey / syrup\", \"Olives\"]}',\n",
      " '{\\'dish\\' : [\"Egg-based dish\", \"Mushroom\"]}',\n",
      " '{\\'dish\\' : [\"Egg-based dish\", \"Toast\"]}',\n",
      " '{\\'dish\\' : [\"Fried potatoes/ sweet potatoes\"]}',\n",
      " '{\\'dish\\' : [\"Fried sweets (e.g. churros, donuts)\"]}',\n",
      " '{\\'dish\\' : [\"Meat soup\"]}',\n",
      " '{\\'dish\\' : [\"Olives\", \"Soft cheese/ cheese spread\", \"Bread/ roll/ '\n",
      " 'baguette\", \"Jam\", \"Plain nuts\"]}',\n",
      " '{\\'dish\\' : [\"Other potatoes/ sweet potatoes\", \"Bread/ roll/ baguette\"]}',\n",
      " '{\\'dish\\' : [\"Pasta with vegetables\"]}',\n",
      " '{\\'dish\\' : [\"Plain rice\"]}',\n",
      " '{\\'dish\\' : [\"Pork\", \"Bread/ roll/ baguette\", \"Pasta with meat\"]}',\n",
      " '{\\'dish\\' : [\"Pork\", \"Other potatoes/ sweet potatoes\", \"Green salad \"]}',\n",
      " '{\\'dish\\' : [\"Pork\"]}',\n",
      " '{\\'dish\\' : [\"Porridge/ oatmeal\"]}',\n",
      " '{\\'dish\\' : [\"Potato crisps\"]}',\n",
      " '{\\'dish\\' : [\"Rice with vegetables\", \"Other vegetable dish\", \"Other '\n",
      " 'vegetable dish\"]}',\n",
      " '{\\'dish\\' : [\"Savoury pie\"]}',\n",
      " '{\\'dish\\' : [\"Savoury snacks\"]}',\n",
      " '{\\'dish\\' : [\"Sweet crepes/ pancakes\", \"Soft cheese/ cheese spread\", '\n",
      " '\"Savoury biscuits\", \"Biscuits\", \"Fried potatoes/ sweet potatoes\"]}',\n",
      " '{\\'dish\\' : [\"Tropical and exotic fruit\", \"Citrus fruit\"]}',\n",
      " '{\\'dish\\' : [\"Tropical and exotic fruit\"]}']\n"
     ]
    }
   ],
   "source": [
    "'''For performance reasons, it is necessary to do some cleaning on the\n",
    "datasets before doing the merge'''\n",
    "# Check the real problem in the column formatted as dictionary\n",
    "pprint(sorted(set(df_logs_1['level2dish_coded'].values)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract the desired value from the column\n",
    "df_logs_1 = clean_dict_column(df=df_logs_1, column='level2dish_coded', key='dish')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 4: solve problem on column \"questions_135633_and_who_are_you_sharing_your_home_with\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['[\"Partner / spouse\", \"Children under 18 years old\"]',\n",
      " '{[\"Children over 18 years old\", \"Parents\"]}',\n",
      " '{[\"Children over 18 years old\", \"Partner / spouse\"]}',\n",
      " '{[\"Children over 18 years old\"]}',\n",
      " '{[\"Children under 18 years old\", \"Children over 18 years old\", \"Parents\"]}',\n",
      " '{[\"Children under 18 years old\", \"Partner / spouse\"]}',\n",
      " '{[\"Children under 18 years old\"]}',\n",
      " '{[\"Friends / Roommates\", \"Other family members\"]}',\n",
      " '{[\"I live alone\"],[\"\"]}',\n",
      " '{[\"I live alone\"]}',\n",
      " '{[\"Other family members\"]}',\n",
      " '{[\"Parents\", \"Other family members\"]}',\n",
      " '{[\"Parents\"]}',\n",
      " '{[\"Partner / spouse\", \"Children 18+\"]}',\n",
      " '{[\"Partner / spouse\", \"Children over 18 years old\"]}',\n",
      " '{[\"Partner / spouse\", \"Children under 18 years old\"]}',\n",
      " '{[\"Partner / spouse\", \"Parents\", \"Other family members\"]}',\n",
      " '{[\"Partner / spouse\"],\"\"}',\n",
      " '{[\"Partner / spouse\"]}']\n"
     ]
    }
   ],
   "source": [
    "from sbees.insecure import clean_braces\n",
    "\n",
    "# check the real problem in the column\n",
    "column_135633 = 'questions_135633_and_who_are_you_sharing_your_home_with'\n",
    "pprint(sorted(set(df_bg_1[column_135633].values)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['[\"Children 18+\", \"Partner / spouse\"]',\n",
      " '[\"Children over 18 years old\", \"Children under 18 years old\", \"Parents\"]',\n",
      " '[\"Children over 18 years old\", \"Parents\"]',\n",
      " '[\"Children over 18 years old\", \"Partner / spouse\"]',\n",
      " '[\"Children over 18 years old\"]',\n",
      " '[\"Children under 18 years old\", \"Partner / spouse\"]',\n",
      " '[\"Children under 18 years old\"]',\n",
      " '[\"Friends / Roommates\", \"Other family members\"]',\n",
      " '[\"I live alone\"]',\n",
      " '[\"Other family members\", \"Parents\", \"Partner / spouse\"]',\n",
      " '[\"Other family members\", \"Parents\"]',\n",
      " '[\"Other family members\"]',\n",
      " '[\"Parents\"]',\n",
      " '[\"Partner / spouse\"]']\n"
     ]
    }
   ],
   "source": [
    "# not all values has the brace problem, so the function created handles that\n",
    "df_bg_1 = clean_braces(df=df_bg_1, column=column_135633)\n",
    "pprint(sorted(set(df_bg_1[column_135633].values)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 5: merge DataFrames\n",
    "\n",
    "PS: the DataFrames were not sorted on merging to keep the supposed order of submission, once project_1 files are supposedly older than project_2 files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sbees.secure import concat_dataframes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the project 2 columns will be used as final column names\n",
    "map_bg_cols = dict(zip(prj_1_cols, prj_2_cols))\n",
    "df_logs = concat_dataframes(df1=df_logs_1, df2=df_logs_2)\n",
    "df_bg = concat_dataframes(df1=df_bg_1, df2=df_bg_2, rename_cols=map_bg_cols)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 6: fix genders spelt / capitalized differently"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sbees.secure import clean_columns_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Demale', 'Female', 'Male', 'Other'}"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "col_gender = 'questions_135556_what_is_your_gender'\n",
    "df_bg = clean_columns_values(df=df_bg, columns=[col_gender])\n",
    "\n",
    "# It shows that \"Demale\" exists, what is clearly a mistake\n",
    "set(df_bg[col_gender].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Female', 'Male', 'Other'}"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Fixing it\n",
    "map_to_replace = {col_gender: {'Demale': 'Female'}}\n",
    "df_bg = clean_columns_values(\n",
    "    df=df_bg,\n",
    "    columns=[col_gender], \n",
    "    replaces=map_to_replace,\n",
    ")\n",
    "set(df_bg[col_gender].values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 7: set all location names to codes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sbees.secure import fix_country_columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['FR', 'MX', 'South Africa', 'UK', 'US', 'United Kingdom', 'ZA']"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 1º: verify the severity of the problem\n",
    "sorted(set(df_logs['location_name'].values))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['FR', 'GB', 'MX', 'US', 'ZA']"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 2ª: fix the problem - small note on conversion UK -> GB as this is the ISO\n",
    "df_logs = fix_country_columns(df=df_logs, columns=['location_name'])\n",
    "sorted(set(df_logs['location_name'].values))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 8: merge duplicated columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sbees.secure import merge_string_columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['LSM_group1',\n",
      " 'questions_134340_what_do_you_do_to_try_to_stay_healthy_tick_all_that_apply',\n",
      " 'questions_134341_what_drives_you_to_have_a_healthy_lifestyle_you_may_select_up_to_3',\n",
      " 'questions_134342_do_you_try_to_eat_or_drink_in_any_of_the_following_ways',\n",
      " 'questions_134344_which_of_the_following_steps_if_any_are_you_taking_to_eat_drink_more_healthily',\n",
      " 'questions_134347_how_regularly_do_you_exercise_you_can_be_honest_it_s_just_between_us',\n",
      " 'questions_134348_which_types_of_exercise_do_you_enjoy_doing_tick_all_that_apply',\n",
      " 'questions_134356_do_you_do_any_of_the_following_to_help_you_sleep_tick_all_that_apply',\n",
      " 'questions_134357_and_do_you_try_to_avoid_certain_things_in_the_hour_before_you_go_to_sleep_tick_all_that_apply',\n",
      " 'questions_135556_what_is_your_gender',\n",
      " 'questions_135557_how_old_are_you',\n",
      " 'questions_135613_who_do_you_share_your_home_with',\n",
      " 'questions_224112_149402_210209_including_yourself_how_many_people_do_you_share_your_home_with',\n",
      " 'user_id']\n"
     ]
    }
   ],
   "source": [
    "# Verify the existence of duplicated columns on background DataFrame\n",
    "pprint(sorted(set(df_bg.columns)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['level2dish_coded',\n",
      " 'location_name',\n",
      " 'questions_134999_where_are_you_eating_at_the_moment',\n",
      " 'questions_134999_where_are_you_eating_at_the_moment.1',\n",
      " 'questions_135000_which_of_the_following_best_describes_where_you_got_your_food_from_today',\n",
      " 'questions_135004_what_else_if_anything_are_you_doing_while_eating_your_food_for_example_are_you_also_watching_tv_checking_social_media_cooking_etc_coded',\n",
      " 'questions_135005_who_are_you_with',\n",
      " 'questions_135019_what_are_the_main_reasons_that_you_chose_this_food_right_now_if_you_are_having_more_than_one_item_please_write_about_the_main_item_you_are_having_please_be_as_detailed_as_you_can_coded',\n",
      " 'questions_135020_what_other_food_if_any_did_you_consider_having_instead_coded',\n",
      " 'questions_135022_who_prepared_the_food',\n",
      " 'questions_223649_149206_210007_finally_which_best_describes_the_food_you_are_having_today',\n",
      " 'submission_id',\n",
      " 'submission_timestamp',\n",
      " 'user_id']\n"
     ]
    }
   ],
   "source": [
    "# and in logs DataFrame\n",
    "pprint(sorted(set(df_logs.columns)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''This shows that the questions_134999_where_are_you_eating_at_the_moment\n",
    "column is duplicated on df_logs and will need to be merged, and now it can\n",
    "be done'''\n",
    "df_logs = merge_string_columns(\n",
    "    df=df_logs,\n",
    "    column_a='questions_134999_where_are_you_eating_at_the_moment',\n",
    "    column_b='questions_134999_where_are_you_eating_at_the_moment.1',\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 9: merge duplicated values on specific columns\n",
    "\n",
    "First is necessary to understand which columns will need to be cleaned,\n",
    "and for that specific case, as it is going to be a massive cleaning,\n",
    "discover dynamically which columns need it would not be a safe / optimal\n",
    "choice, so the columns that needs the treatment were verified \"simply\" by\n",
    "looking at the data and checking the ones that have an structure of a list.\n",
    "The columns were then mapped and placed into an asset file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from sbees.insecure import remove_duplicated_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'background': ['questions_135613_who_do_you_share_your_home_with',\n",
      "                'questions_134340_what_do_you_do_to_try_to_stay_healthy_tick_all_that_apply',\n",
      "                'questions_134341_what_drives_you_to_have_a_healthy_lifestyle_you_may_select_up_to_3',\n",
      "                'questions_134342_do_you_try_to_eat_or_drink_in_any_of_the_following_ways',\n",
      "                'questions_134344_which_of_the_following_steps_if_any_are_you_taking_to_eat_drink_more_healthily',\n",
      "                'questions_134348_which_types_of_exercise_do_you_enjoy_doing_tick_all_that_apply',\n",
      "                'questions_134356_do_you_do_any_of_the_following_to_help_you_sleep_tick_all_that_apply',\n",
      "                'questions_134357_and_do_you_try_to_avoid_certain_things_in_the_hour_before_you_go_to_sleep_tick_all_that_apply'],\n",
      " 'logs': ['questions_135005_who_are_you_with',\n",
      "          'questions_135004_what_else_if_anything_are_you_doing_while_eating_your_food_for_example_are_you_also_watching_tv_checking_social_media_cooking_etc_coded',\n",
      "          'level2dish_coded',\n",
      "          'questions_135019_what_are_the_main_reasons_that_you_chose_this_food_right_now_if_you_are_having_more_than_one_item_please_write_about_the_main_item_you_are_having_please_be_as_detailed_as_you_can_coded',\n",
      "          'questions_135020_what_other_food_if_any_did_you_consider_having_instead_coded']}\n"
     ]
    }
   ],
   "source": [
    "with open(f'{assets_folder}list_like_columns_map.json') as file:\n",
    "    list_like_columns_map = json.load(file)\n",
    "pprint(list_like_columns_map)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_bg = remove_duplicated_values(\n",
    "    df=df_bg,\n",
    "    list_like_columns=list_like_columns_map['background'],\n",
    ")\n",
    "\n",
    "df_logs = remove_duplicated_values(\n",
    "    df=df_logs,\n",
    "    list_like_columns=list_like_columns_map['logs'],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 10: save the DataFrames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Could be also saved as XLSX, Parquet, etc.\n",
    "df_bg.to_csv('output/background_dataset.csv')\n",
    "df_logs.to_csv('output/logs_dataset.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All done :)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.7 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "7307b88a982307dd8325056769723498162fc2b0abaa94503f3c99a51b98cd48"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
